7 March 2021

Experimenting with coordinating my tigercpu director files with this repository.



3 January 2020

I'm trying to work with Yilun's pipeline code, and am getting stuck in working with .h5 files.  On feynman,
I cannot read or write h5py files.   Here is a typical error message:
   
>>> f = h5py.File("htest.h5","w")
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/mnt/act3/users/treu/miniconda2/lib/python2.7/site-packages/h5py/_hl/files.py", line 312, in __init__
    fid = make_fid(name, mode, userblock_size, fapl, swmr=swmr)
  File "/mnt/act3/users/treu/miniconda2/lib/python2.7/site-packages/h5py/_hl/files.py", line 148, in make_fid
    fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl)
  File "h5py/_objects.pyx", line 54, in h5py._objects.with_phil.wrapper
  File "h5py/_objects.pyx", line 55, in h5py._objects.with_phil.wrapper
  File "h5py/h5f.pyx", line 98, in h5py.h5f.create
IOError: Unable to create file (unable to lock file, errno = 37, error message = 'No locks available')

So, instead,  I'll to create a full feature set (all ~11+3 features) from some new TODs, using my (older than 
Yilun's pipeline) code.   But...
I took one more try at h5py.   And tried updating h5py, which I got sucked into;  indeed it updated a lot of code.  Included
in what was updated was python!   It downloaded and installed python 3.7x  So now I'm fussing with trying to make the rest of 
my feynman code compatible;  this necessitated changes in .bashrc etc etc !







15 Decemberr 2019
anal_13Dec.py and anal_15Dec.py are on simons1 in the treu9 folder.
anal_13Dec.py has an neural network section at the end which produces accuracy around 75% using 3 or 4 features 
    feat1              = np.load('feature1_train_tes_30Nov19.npy')
feat2              = np.load('feature2_train_tes_30Nov19.npy')
feat3              = np.load('feature3_train_tes_30Nov19.npy')
feat4              = np.load('feature4_train_tes_30Nov19.npy') 

anal_15Dec.py was constructed to test Outlier detection methods (such as https://scikit-learn.org/stable/modules/outlier_detection.html).
ToDo's: 
(1) test the old classifiers such as in anal_15Dec.py using test data from a different part of the same season as the training set; or better yet, a different season.
(2) try out Outlier Detection methodology.


3 December 2019
    It's good that the neural network code is now working.  72.5% accuracy is "interesting" but not good enough.
    Next steps include:  
(a)  checking that the math is correct and the proper TODs are being analyzed;
(b)  adding back in additional features such as Loic parameters that do not require human thresh-holding
(c)  then...  IF better accuracy is achieved, then graduate to checking on TODs not part of the group used for training.




2 December 2019

On feynman,  I ran the processing code using Yilun's "tes" screened TODs.
Then, on simons1,  I read these into anal_28Nov.py:

feat1              = np.load('feature1_train_tes_30Nov19.npy')
feat2              = np.load('feature2_train_tes_30Nov19.npy')
feat3              = np.load('feature3_train_tes_30Nov19.npy')
feat4              = np.load('feature4_train_tes_30Nov19.npy')  (what's this feature?  Is it "CV" ?)

fp = {'feat1':feat1, 'feat2':feat2, 'feat3':feat3, 'feat4':feat4}
results = np.load('result_train_tes_30Nov19.npy')



Here are the results:

StandardScaler is on.
(' shape of X_train_scaled =  ', (48336, 4))
X-test shape: (16112, 4)
x-train shape: (48336, 4)
y_test shape: (16112,)
y-train shape: (48336,)

('nNeighbors =  ', 3)
 
 
('shape of y_test = ', (16112,), '    shape of y_pred = ', (16112,), '\n')

Results:  KNN Model




True-positives  	 1503
True-negatives  	 9802
False-positives 	 1885
False-negatives 	 2922



total samples  	 16112 


True-positives:  9.3 %
True-negatives:  60.8 %
False-positives: 11.7 %
False-negatives: 18.1 %
 
Accuracy:        70.2 %
Precision:       44.4 %
Sensitivity:     34.0 %
 

XGB Results: 
Accuracy of xgb on test set:     0.7286494538232373
 Start of PCA segment....  shape of X_train_scaled) is : (48336, 4)
 shape of y_train  (48336,)
 
  PCA transformed 
PCA component shape : (4, 4)
('shape of X_pca ', (48336, 4))
 X_pca[0] shape : (4,)
      [1]         (4,)
      [2]         (4,)
      [3]         (4,)
shape of comp1 (48336,)
shape of comp2 (48336,)
shape of comp3 (48336,)
shape of comp4 (48336,)
shape of truth (48336,)





shape (X_train) (48336, 4)
 shape of features_train:   (48336, 5)
 shape of Dplot :   (48336, 5)





 STARTING NEURAL NETWORK SECTION 




Train on 48336 samples, validate on 16112 samples

48336/48336 [==============================] - 1s 28us/step - loss: 0.5807 - acc: 0.7326 - val_loss: 0.5880 - val_acc: 0.7254
Epoch 25/25
48336/48336 [==============================] - 1s 28us/step - loss: 0.5807 - acc: 0.7326 - val_loss: 0.5880 - val_acc: 0.7254






30 November 2019

On feynman, I am running processing_general.py and I intend to use PCA to reduce features to 3 or 4 and then run neural net (on Simons1 where 
it is available (keras).



28 November 2019

On Simons1 server, I am working with python3 code.   I copied anal_fullset.py code from feynman ~/mnt/act3/users/treu9/data, converted to 
python3.  I copied results.npy and featx_train.npy where x= 1, 2 and 3.   See feynman code from Feb., 2019 to backtrack and find which
TODs these represent.   

I copied anal_21Nov.py (on simons1) over to anal_28Nov.py and ran the neural network code using only features 1, 2 and 3.  
This converged in just 2 epochs (!) to an accuracy of 78%.  
Seems I need more features.  



13 Feb 2019

I finally got Yilun's mlpipe pipeline working.  Here are a couple of initial results, using his dataset.h5 data and his initial set of features which are only Loic's original pickle parameters, with no "human intervention":

== VALIDATION RESULTS: ==

  epoch    batch  model              loss      base    accuracy    tp    tn    fp    fn    precision    recall        f1     time/s
-------  -------  --------------  -------  --------  ----------  ----  ----  ----  ----  -----------  --------  --------  ---------
      0        0  KNNModel-3      2.08539  0.422877    0.939623  6877  9059   729   295     0.904155  0.958868  0.930708  0.237996
      0        0  DecisionTree    1.69642  0.422877    0.950884  6874  9253   535   298     0.927791  0.95845   0.942871  0.0137727
      0        0  XGBoost         1.35226  0.422877    0.960849  7150  9146   642    22     0.917608  0.996933  0.955627  0.0628161
      0        0  RandomForest-5  1.40317  0.422877    0.959375  7151  9120   668    21     0.914567  0.997072  0.954039  0.0773206

Now with nearest neighbors = 7 in the KKN Model:

== VALIDATION RESULTS: ==

  epoch    batch  model              loss      base    accuracy    tp    tn    fp    fn    precision    recall        f1     time/s
-------  -------  --------------  -------  --------  ----------  ----  ----  ----  ----  -----------  --------  --------  ---------
      0        0  DecisionTree    1.77584  0.422877    0.948585  6831  9257   531   341     0.927873  0.952454  0.940003  0.0162508
      0        0  XGBoost         1.35226  0.422877    0.960849  7152  9144   644    20     0.917394  0.997211  0.955639  0.0583732
      0        0  KNNModel-7      1.80232  0.422877    0.947818  7048  9027   761   124     0.902548  0.982711  0.940925  0.225894
      0        0  RandomForest-5  1.45408  0.422877    0.957901  7165  9081   707     7     0.910188  0.999024  0.952539  0.0734301

Here is nn = 10 for the KNN Model:
      0        0  KNNModel-10     1.68828  0.422877    0.95112   7101  9030   758    71     0.90355   0.9901    0.944847  0.255814


7 Feb 2019

Yilun says he is now using analyse.py to generate computer-only pickle parameters;  then using process_cuts.py (?? right file ) it runs the AI routine of choice with whatever parameters are selected.   Yilun has streamlined Loic's code, too.    This generated the h5 data file which can run in the AI pipeline Yilun earlier created.



----------------------------------------------------------------------------------------------------------------------------------
below this line the entries are in reverse time order:



2/3/2019
  Trying proc_tes_2Feb.py,   as more recent attempts of combining Yilun's tes filtered TOD list with my feature set ran into trouble.  
  In this version of the code, I've got feature1, 2 and 3, plus the relatively new feature5, which is the weird ~60 sec filter in 
calculation.  I'll run this code overnight and generate feature data with which to try analysis.
  RESULT:   code timed out after 4 hours of running.  
  I timed proc_tes_2Feb.py running through all the features listed just above plus the results (using cuts analysis from moby2); 
it comes out to a little over 2 min for the first TOD of the set.     On the basis of this, I'm now trying "len=20" to go through 
20 TODs in batch mode.  This ought to finish in about 45 min.
  This finished fairly promptly;  but the length of the feature output files was screwy.   I deleted these output .npy files and am running this code again.
  Note that if 20 TODs will run in ~45 min., I could manually concoct four or five copies of the code to tackle different sets of 20 TODs each and run them in parallel manually.  This, the analysis code could simply read all the input files and combine the data into the full 70 or 80 TOD or 100 TOD data set.   Better be carefully about the order of reading and such.
  I figured out that I was appending to the feature arrays in the wrong position within the loops in the code.  I think it works now.  Also, the result array was being constructed incorrectly in this code!   yeeech.    ---->>>  nope:  the features are looking ok, but the result_train_tes_3Feb.npy file is too short !!!  More work to do.  :(
    
4 Feb 2019
    proc_tes_2Feb.py is all messed up with indices !   j count runs too big - why?  k count is off -- why ?    
    
6 Feb 2019
    I rewrote the operating parts of proc_tes_2Feb.py and it seems to work now.
    Generated results and feature1, feature2, feature3, feature5 for 5 TODs (I think...  the test set had 1060 samples in it for the following experiments):
NN = 3
1060 samples
True-positives:  22.3 %
True-negatives:  48.4 %
False-positives: 12.6 %
False-negatives: 16.7 %
 
Accuracy:        70.7 %
Precision:       63.8 %
Sensitivity:     57.1 %

NN = 4
1060 samples
True-positives:  16.5 %
True-negatives:  54.0 %
False-positives: 7.1 %
False-negatives: 22.5 %
 
Accuracy:        70.5 %
Precision:       70.0 %
Sensitivity:     42.4 %

NN = 2
1060 samples
True-positives:  15.5 %
True-negatives:  54.2 %
False-positives: 6.8 %
False-negatives: 23.5 %
 
Accuracy:        69.7 %
Precision:       69.5 %
Sensitivity:     39.7 %

Using ONLY feture5: 
1060 samples
True-positives:  22.1 %
True-negatives:  45.3 %
False-positives: 15.8 %
False-negatives: 16.9 %
 
Accuracy:        67.4 %
Precision:       58.4 %
Sensitivity:     56.7 %

----
Back to feat1, feat2, feat3 only
NN = 3
1060 samples
True-positives:  18.3 %
True-negatives:  46.0 %
False-positives: 15.0 %
False-negatives: 20.7 %
 
Accuracy:        64.3 %
Precision:       55.0 %
Sensitivity:     47.0 %

Recap:
Best accuracy =  70.7% with feat1, feat2, feat3, feat5 avail, NN=6
Lowest FP rate =  6.8% with     ""  ,  NN = 2
Lowest FN rate =  16.7%  with    " ",  NN = 3
Best sensitivity =  57.1%  with  " ",  NN= 3






  
  
  
