13 Feb 2019

I finally got Yilun's mlpipe pipeline working.  Here are a couple of initial results, using his dataset.h5 data and his initial set of features which are only Loic's original pickle parameters, with no "human intervention":

== VALIDATION RESULTS: ==

  epoch    batch  model              loss      base    accuracy    tp    tn    fp    fn    precision    recall        f1     time/s
-------  -------  --------------  -------  --------  ----------  ----  ----  ----  ----  -----------  --------  --------  ---------
      0        0  KNNModel-3      2.08539  0.422877    0.939623  6877  9059   729   295     0.904155  0.958868  0.930708  0.237996
      0        0  DecisionTree    1.69642  0.422877    0.950884  6874  9253   535   298     0.927791  0.95845   0.942871  0.0137727
      0        0  XGBoost         1.35226  0.422877    0.960849  7150  9146   642    22     0.917608  0.996933  0.955627  0.0628161
      0        0  RandomForest-5  1.40317  0.422877    0.959375  7151  9120   668    21     0.914567  0.997072  0.954039  0.0773206

Now with nearest neighbors = 7 in the KKN Model:

== VALIDATION RESULTS: ==

  epoch    batch  model              loss      base    accuracy    tp    tn    fp    fn    precision    recall        f1     time/s
-------  -------  --------------  -------  --------  ----------  ----  ----  ----  ----  -----------  --------  --------  ---------
      0        0  DecisionTree    1.77584  0.422877    0.948585  6831  9257   531   341     0.927873  0.952454  0.940003  0.0162508
      0        0  XGBoost         1.35226  0.422877    0.960849  7152  9144   644    20     0.917394  0.997211  0.955639  0.0583732
      0        0  KNNModel-7      1.80232  0.422877    0.947818  7048  9027   761   124     0.902548  0.982711  0.940925  0.225894
      0        0  RandomForest-5  1.45408  0.422877    0.957901  7165  9081   707     7     0.910188  0.999024  0.952539  0.0734301

Here is nn = 10 for the KNN Model:
      0        0  KNNModel-10     1.68828  0.422877    0.95112   7101  9030   758    71     0.90355   0.9901    0.944847  0.255814


7 Feb 2019

Yilun says he is now using analyse.py to generate computer-only pickle parameters;  then using process_cuts.py (?? right file ) it runs the AI routine of choice with whatever parameters are selected.   Yilun has streamlined Loic's code, too.    This generated the h5 data file which can run in the AI pipeline Yilun earlier created.



----------------------------------------------------------------------------------------------------------------------------------
below this line the entries are in reverse time order:



2/3/2019
  Trying proc_tes_2Feb.py,   as more recent attempts of combining Yilun's tes filtered TOD list with my feature set ran into trouble.  
  In this version of the code, I've got feature1, 2 and 3, plus the relatively new feature5, which is the weird ~60 sec filter in 
calculation.  I'll run this code overnight and generate feature data with which to try analysis.
  RESULT:   code timed out after 4 hours of running.  
  I timed proc_tes_2Feb.py running through all the features listed just above plus the results (using cuts analysis from moby2); 
it comes out to a little over 2 min for the first TOD of the set.     On the basis of this, I'm now trying "len=20" to go through 
20 TODs in batch mode.  This ought to finish in about 45 min.
  This finished fairly promptly;  but the length of the feature output files was screwy.   I deleted these output .npy files and am running this code again.
  Note that if 20 TODs will run in ~45 min., I could manually concoct four or five copies of the code to tackle different sets of 20 TODs each and run them in parallel manually.  This, the analysis code could simply read all the input files and combine the data into the full 70 or 80 TOD or 100 TOD data set.   Better be carefully about the order of reading and such.
  I figured out that I was appending to the feature arrays in the wrong position within the loops in the code.  I think it works now.  Also, the result array was being constructed incorrectly in this code!   yeeech.    ---->>>  nope:  the features are looking ok, but the result_train_tes_3Feb.npy file is too short !!!  More work to do.  :(
    
4 Feb 2019
    proc_tes_2Feb.py is all messed up with indices !   j count runs too big - why?  k count is off -- why ?    
    
6 Feb 2019
    I rewrote the operating parts of proc_tes_2Feb.py and it seems to work now.
    Generated results and feature1, feature2, feature3, feature5 for 5 TODs (I think...  the test set had 1060 samples in it for the following experiments):
NN = 3
1060 samples
True-positives:  22.3 %
True-negatives:  48.4 %
False-positives: 12.6 %
False-negatives: 16.7 %
 
Accuracy:        70.7 %
Precision:       63.8 %
Sensitivity:     57.1 %

NN = 4
1060 samples
True-positives:  16.5 %
True-negatives:  54.0 %
False-positives: 7.1 %
False-negatives: 22.5 %
 
Accuracy:        70.5 %
Precision:       70.0 %
Sensitivity:     42.4 %

NN = 2
1060 samples
True-positives:  15.5 %
True-negatives:  54.2 %
False-positives: 6.8 %
False-negatives: 23.5 %
 
Accuracy:        69.7 %
Precision:       69.5 %
Sensitivity:     39.7 %

Using ONLY feture5: 
1060 samples
True-positives:  22.1 %
True-negatives:  45.3 %
False-positives: 15.8 %
False-negatives: 16.9 %
 
Accuracy:        67.4 %
Precision:       58.4 %
Sensitivity:     56.7 %

----
Back to feat1, feat2, feat3 only
NN = 3
1060 samples
True-positives:  18.3 %
True-negatives:  46.0 %
False-positives: 15.0 %
False-negatives: 20.7 %
 
Accuracy:        64.3 %
Precision:       55.0 %
Sensitivity:     47.0 %

Recap:
Best accuracy =  70.7% with feat1, feat2, feat3, feat5 avail, NN=6
Lowest FP rate =  6.8% with     ""  ,  NN = 2
Lowest FN rate =  16.7%  with    " ",  NN = 3
Best sensitivity =  57.1%  with  " ",  NN= 3






  
  
  
